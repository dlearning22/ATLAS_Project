# -*- coding: utf-8 -*-
"""R2AU-net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z8X3f-mEjlg99PDFtMstIpo-3zjDkri0

# **Attention Recurrent-Residual U-net**
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import init

"""# **Recurrent Block**"""

class Recurrent_block(nn.Module):
    def __init__(self,
                 out_ch,
                 t=2,
                 num_groups=8):
        super(Recurrent_block, self).__init__()

        self.t = t
        self.out_ch = out_ch
        self.conv = nn.Sequential(
            nn.Conv3d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),
            nn.GroupNorm(num_groups,out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        for i in range(self.t):
            if i == 0:
                x = self.conv(x)
            out = self.conv(x + x)
        return out

"""# **Recurrent-Residual Block**"""

class RRCNN_block(nn.Module):
    def __init__(self,
                 ch_in,
                 ch_out,
                 t=2):
        super(RRCNN_block,self).__init__()
        self.RCNN = nn.Sequential(
            Recurrent_block(ch_out,t=t),
            Recurrent_block(ch_out,t=t)
        )
        self.Conv_1x1 = nn.Conv3d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)

    def forward(self,x):
        x = self.Conv_1x1(x)
        x1 = self.RCNN(x)
        return x+x1

"""Attention Block"""

class Attention_block(nn.Module):
    def __init__(self,
                 F_g,
                 F_l,
                 F_int,
                 num_groups=8):
        super(Attention_block,self).__init__()
        self.W_g = nn.Sequential(
            nn.Conv3d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),
            nn.GroupNorm(num_groups,F_int)
            )
        
        self.W_x = nn.Sequential(
            nn.Conv3d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),
            nn.GroupNorm(num_groups,F_int)
        )

        self.psi = nn.Sequential(
            nn.Conv3d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),
            nn.GroupNorm(num_groups,1),
            nn.Sigmoid()
        )
        
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self,g,x):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        psi = self.relu(g1+x1)
        psi = self.psi(psi)

        return x*psi

"""# **New Attention Gate**"""

class GridAttentionGateLocal3D(nn.Module):

    def __init__(self, Fg, Fl, Fint, learn_upsampling=False, batchnorm=False):
        super(GridAttentionGateLocal3D, self).__init__()

        if batchnorm:
            self.Wg = nn.Sequential(
                nn.Conv3d(Fg, Fint, kernel_size=1, stride=1, padding=0, bias=True),
                nn.BatchNorm3d(Fint)
            )
            self.Wx = nn.Sequential(
                nn.Conv3d(Fl, Fint, kernel_size=1, stride=1, padding=0, bias=False),
                nn.BatchNorm3d(Fint),
                nn.MaxPool3d(2)
            )

            self.y = nn.Sequential(
                nn.Conv3d(in_channels=Fint, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True),
                nn.BatchNorm3d(1)
            )

        else:
            self.Wg = nn.Conv3d(Fg, Fint, kernel_size=1, stride=1, padding=0, bias=True)
            self.Wx = nn.Sequential(
                nn.Conv3d(Fl, Fint, kernel_size=1, stride=1, padding=0, bias=False),
                nn.MaxPool3d(2)
            )

            self.y = nn.Conv3d(in_channels=Fint, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)

        self.out = nn.Sequential(
            nn.Conv3d(in_channels=Fl, out_channels=Fl, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm3d(Fl),
        )

    def forward(self, xl, g):

        xl_size_orig = xl.size()
        xl_ = self.Wx(xl)

        g = self.Wg(g)

        relu = F.relu(xl_ + g, inplace=True)
        y = self.y(relu)
        sigmoid = torch.sigmoid(y)

        upsampled_sigmoid = F.interpolate(sigmoid, size=xl_size_orig[2:], mode='trilinear', align_corners=False)

        # scale features with attention
        attention = upsampled_sigmoid.expand_as(xl)

        return self.out(attention * xl)

def decoder(in_channels,
                out_channels,
                kernel_size,
                stride=1,
                padding=0,
                output_padding=0,
                bias=True,
                relu=True,
               num_groups=8):
        layer = [
            nn.ConvTranspose3d(in_channels,
                               out_channels,
                               kernel_size,
                               stride=stride,
                               padding=padding,
                               output_padding=output_padding,
                               bias=bias),
        ]
        if relu: # When is ReLu not used in this case?
            layer.append(nn.GroupNorm(num_groups, out_channels))
            layer.append(nn.ReLU())
        layer = nn.Sequential(*layer)
        return layer

"""# **R2AU-net** """

class UNet3D(nn.Module):
    def __init__(self,
                 in_channel,
                 n_classes,
                 use_bias=True,
                 inplanes=32,
                 num_groups=8,
                 t=2,
                 output_ch=1): # Change these according to later code?
        self.in_channel = in_channel
        self.n_classes = n_classes
        self.inplanes = inplanes
        self.num_groups = num_groups
        planes = [inplanes * int(pow(2, i)) for i in range(0, 5)] # int()= integer, pow()=power, a list is created: [32,64,128,256,512,1024]         
        super(UNet3D,self).__init__()
        
        self.Upsample = nn.Upsample(scale_factor=2)

        self.ec0 = RRCNN_block(in_channel,planes[1],t=t)

        self.ec1 = RRCNN_block(planes[1],planes[2],t=t)
        
        self.ec1_2 = RRCNN_block(planes[2],planes[2],t=t)
        
        self.ec2 = RRCNN_block(planes[2],planes[3],t=t)
        
        self.ec2_2 = RRCNN_block(planes[3],planes[3],t=t)

        self.ec3 = RRCNN_block(planes[3],planes[4],t=t)

        self.ec3_2 = RRCNN_block(planes[4],planes[4],t=t)

        self.maxpool = nn.MaxPool3d(2)

        self.dc3 = RRCNN_block(planes[4],planes[4],t=t)

        self.dc3_2 = RRCNN_block(planes[4],planes[4],t=t)

        self.up3 = decoder(planes[4],
                                planes[3],
                                kernel_size=2,
                                stride=2,
                                bias=use_bias)

        self.dc2 = RRCNN_block(planes[4],planes[3],t=t)

        self.dc2_2 = RRCNN_block(planes[3],planes[3],t=t)

        self.up2 = decoder(planes[3],
                                planes[2],
                                kernel_size=2,
                                stride=2,
                                bias=use_bias)

        self.dc1 = RRCNN_block(planes[3],planes[2],t=t)

        self.dc1_2 = RRCNN_block(planes[2],planes[2],t=t)

        self.up1 = decoder(planes[2],
                                planes[1],
                                kernel_size=2,
                                stride=2,
                                bias=use_bias)

        self.dc0a = RRCNN_block(planes[2],planes[1],t=t)

        self.dc0b = RRCNN_block(planes[1],n_classes,t=t)

#############################################################################################

        self.Att4 = Attention_block(F_g= planes[3],F_l= planes[3],F_int= planes[2])
        
        self.Att3 = Attention_block(F_g= planes[2],F_l= planes[2],F_int= planes[1])
       
        self.Att2 = Attention_block(F_g= planes[1],F_l= planes[1],F_int= planes[0])
      

##########################################################################################

        # Initializing weights in encoder and decoder according to type of layer:
        for m in self.modules():
            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d) : 
                nn.init.kaiming_normal_(m.weight,
                                        mode='fan_out',
                                        nonlinearity='relu')
            elif isinstance(m, nn.GroupNorm):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

###################################################################################


    def forward(self, x):
        e0 = self.ec0(x) #64
        e1 = self.ec1_2(self.ec1(self.maxpool(e0))) #128
        e2 = self.ec2_2(self.ec2(self.maxpool(e1))) #256
        e3 = self.ec3_2(self.ec3(self.maxpool(e2))) #512
        d3 = self.up3(self.dc3_2(self.dc3(e3))) #256
        a4 = self.Att4(g= d3, x= e2 ) #256
        if d3.size()[2:] != a4.size()[2:]:
            d3 = F.interpolate(d3,
                               a4.size()[2:],
                               mode='trilinear',
                               align_corners=False)
        d3 = torch.cat((d3, a4), 1) #512
        d2 = self.up2(self.dc2_2(self.dc2(d3))) #128
        a3 = self.Att3(g= d2, x= e2 ) #128
        if d2.size()[2:] != a3.size()[2:]:
            d2 = F.interpolate(d2,
                               a3.size()[2:],
                               mode='trilinear',
                               align_corners=False)
        d2 = torch.cat((d2, a3), 1) #128
        d1 = self.up1(self.dc1_2(self.dc1(d2))) #32
        a2 = self.Att3(g= d1, x= e0 )
        if d1.size()[2:] != a2.size()[2:]:
            d1 = F.interpolate(d1,
                               a2.size()[2:],
                               mode='trilinear',
                               align_corners=False)
        d1 = torch.cat((d1, e0), 1) #32
        d0 = self.dc0b(self.dc0a(d1)) #1
        return d0


        

###########################################################################################

# -*- coding: utf-8 -*-
"""Experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A_0Myw2mX4Gp-s6xvAPmvQB32vfuZ8U-
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import init

""# **Recurrent Block**"""

class Recurrent_block(nn.Module):
    def __init__(self,
                 out_ch,
                 t=2):
        super(Recurrent_block, self).__init__()

        self.t = t
        self.out_ch = out_ch
        self.conv = nn.Sequential(
            nn.Conv3d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),
            nn.BatchNorm3d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        for i in range(self.t):
            if i == 0:
                x = self.conv(x)
            out = self.conv(x + x)
        return out

"""# **Recurrent-Residual Block**"""

class RRCNN_block(nn.Module):
    def __init__(self,
                 ch_in,
                 ch_out,
                 t=2):
        super(RRCNN_block,self).__init__()
        self.RCNN = nn.Sequential(
            Recurrent_block(ch_out,t=t),
            Recurrent_block(ch_out,t=t)
        )
        self.Conv_1x1 = nn.Conv3d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)

    def forward(self,x):
        x = self.Conv_1x1(x)
        x1 = self.RCNN(x)
        return x+x1


"""# **Attention Block**"""

class Attention_block(nn.Module):
    def __init__(self,
                 F_g,
                 F_l,
                 F_int):
        super(Attention_block,self).__init__()
        self.W_g = nn.Sequential(
            nn.Conv3d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),
            nn.BatchNorm3d(F_int)
            )
        
        self.W_x = nn.Sequential(
            nn.Conv3d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),
            nn.BatchNorm3d(F_int)
        )

        self.psi = nn.Sequential(
            nn.Conv3d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),
            nn.BatchNorm3d(1),
            nn.Sigmoid()
        )
        
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self,g,x):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        psi = self.relu(g1+x1)
        psi = self.psi(psi)

        return x*psi
###########################################################################################################
class decoder2(nn.Module):
    def __init__(self,ch_in,ch_out):
        super(decoder,self).__init__()
        self.up = nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.Conv3d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),
		    nn.BatchNorm3d(ch_out),
			nn.ReLU(inplace=True)
        )

    def forward(self,x):
        x = self.up(x)
        return x
#########################################################################################################
def decoder(self,
                in_channels,
                out_channels,
                kernel_size,
                stride=1,
                padding=0,
                output_padding=0,
                bias=True,
                relu=True):
        layer = [
            nn.ConvTranspose3d(in_channels,
                               out_channels,
                               kernel_size,
                               stride=stride,
                               padding=padding,
                               output_padding=output_padding,
                               bias=bias),
        ]
        if relu:
            layer.append(nn.BatchNorm3D(out_channels))
            layer.append(nn.ReLU())
        layer = nn.Sequential(*layer)
        return layer
**************************************************************************************************************************
"""# **R2AU-net** """

class UNet3D(nn.Module):
    def __init__(self,
                 in_channel,
                 n_classes,
                 use_bias=True,
                 inplanes=32,
                 num_groups=8,
                 t=2,
                 output_ch=1): # Change these according to later code?
        self.in_channel = in_channel
        self.n_classes = n_classes
        self.inplanes = inplanes
        self.num_groups = num_groups
        planes = [inplanes * int(pow(2, i)) for i in range(0, 5)] # int()= integer, pow()=power, a list is created: [32,64,128,256,512,1024]         
        super(UNet3D,self).__init__()
        
        self.Upsample = nn.Upsample(scale_factor=2)

        self.ec0 = RRCNN_block(in_channel,planes[1],t=t)

        self.ec1 = RRCNN_block(planes[1],planes[2],t=t)
        
        self.ec1_2 = RRCNN_block(planes[2],planes[2],t=t)
        
        self.ec2 = RRCNN_block(planes[2],planes[3],t=t)
        
        self.ec2_2 = RRCNN_block(planes[3],planes[3],t=t)

        self.ec3 = RRCNN_block(planes[3],planes[4],t=t)

        self.ec3_2 = RRCNN_block(planes[4],planes[4],t=t)

        self.maxpool = nn.MaxPool3d(2)

        self.dc3 = RRCNN_block(planes[4],planes[4],t=t)

        self.dc3_2 = RRCNN_block(planes[4],planes[4],t=t)

        self.up3 = decoder(planes[4],
                                planes[3],kernel_size=2,
                                stride=2)

        self.dc2 = RRCNN_block(planes[4],planes[3],t=t)

        self.dc2_2 = RRCNN_block(planes[3],planes[3],t=t)

        self.up2 = decoder(planes[3],
                                planes[2],kernel_size=2,
                                stride=2)

        self.dc1 = RRCNN_block(planes[3],planes[2],t=t)

        self.dc1_2 = RRCNN_block(planes[2],planes[2],t=t)

        self.up1 = decoder(planes[2],
                                planes[1],kernel_size=2,
                                stride=2)

        self.dc0a = RRCNN_block(planes[2],planes[1],t=t)

        self.dc0b = RRCNN_block(planes[1],n_classes,t=t)

###########################################################################################

        self.Att4 = Attention_block(F_g= planes[3],F_l= planes[3],F_int= planes[2])
        
        self.Att3 = Attention_block(F_g= planes[2],F_l= planes[2],F_int= planes[1])
       
        self.Att2 = Attention_block(F_g= planes[1],F_l= planes[1],F_int= planes[0])

        self.Conv1 = nn.Conv3d(planes[1],n_classes,kernel_size=1,stride=1,padding=0)

      

##########################################################################################

        # Initializing weights in encoder and decoder according to type of layer:
        for m in self.modules():
            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d) : 
                nn.init.kaiming_normal_(m.weight,
                                        mode='fan_out',
                                        nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

###################################################################################


    def forward(self, x):
      # Encoder
        e0 = self.ec0(x) # 1
        e1 = self.maxpool(e0)
        e1 = self.ec1(e1) # 2
        e2 = self.maxpool(e1)
        e2 = self.ec2(e2) # 3
        e3 = self.maxpool(e2)
        e3 = self.ec3(e3) # 4

      # Decoder and Attention

        d2 = self.up3(e3) # 3
        a3 = self.Att4(g= d2, x= e2 ) # 3
        if d2.size()[2:] != a3.size()[2:]:
            d2 = F.interpolate(d2,
                               a3.size()[2:],
                               mode='trilinear',
                               align_corners=False)
        d2 = torch.cat((d2, a3), 1) # 3
        d2 = self.dc2(d2)

        d3 = self.up2(d2) # 2
        a2 = self.Att3(g= d3, x= e1 ) # 2 
        if d3.size()[2:] != a2.size()[2:]:
            d1 = F.interpolate(d1,
                               a2.size()[2:],
                               mode='trilinear',
                               align_corners=False)
        d3 = torch.cat((d3, a2), 1) # 2
        d3= self.dc1(d3)

        d4 = self.up1(d3) # 1
        a1 = self.Att2(g= d4, x= e0 ) # 1 
        if d4.size()[2:] != a1.size()[2:]:
            d1 = F.interpolate(d1,
                               a1.size()[2:],
                               mode='trilinear',
                               align_corners=False)
        d4 = torch.cat((d4, a1), 1)  
        d4= self.dc0a(d4) # 1
        d5 = self.Conv1(d4)

        return d5
